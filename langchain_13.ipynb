{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anumit-web/python-interview-2024/blob/main/machine%20learning/Unsupervised_Learning_clustering_type_iris_flower_clustering_Gaussian_mixture_model_(GMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lhvb-UXLbF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomuVIUl9uY6"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Shee7JuzVF"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "\n",
        "# FewShotPromptTemplate\n",
        "\n",
        "# ChatPromptTemplate"
      ],
      "metadata": {
        "id": "99VCqplEvvAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating PromptTemplate with LLMs"
      ],
      "metadata": {
        "id": "8XwHCnNavxns"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1vTaKl9oHQ"
      },
      "source": [
        "# Chapter 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4QaixAfPyF"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKa4SWV84DMv"
      },
      "source": [
        "# Python Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pRBaqLz9atZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFXpB4wv-5-1",
        "outputId": "5699722c-6c02-4358-89ec-64f199975fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0djESK29att",
        "outputId": "d2024fd1-545c-498c-e1ae-2a9edcbbb526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfFMli_AEOK2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "45c5d79e-81bd-4092-cf9e-44a8226d23f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2024-11-12  07:09'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 276
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "tz = timezone('EST')\n",
        "datetime.now(tz).strftime(\"%Y-%m-%d  %H:%M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDoARvnvU2L"
      },
      "source": [
        "---\n",
        "---\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR391vUc7l-N"
      },
      "source": [
        "# Python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfbOedi1WHO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SZd9nC1wLm3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import langchain\n",
        "except:\n",
        "  !pip install langchain\n",
        "  import langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iCA0l5hxjB8"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-community langchain-core --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n5IXOCNxppN"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade langchain --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -qU langchain-openai --quiet"
      ],
      "metadata": {
        "id": "_zVBLvkpHoUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWiheCyd1Yc0"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-cohere --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-huggingface --quiet"
      ],
      "metadata": {
        "id": "z_eJLyvxN3ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0RW7lT_UxHE"
      },
      "outputs": [],
      "source": [
        "# ! pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01GEQvpTUyyp"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-core langgraph>0.2.27 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck7bfWqemvdt"
      },
      "outputs": [],
      "source": [
        "# ! pip install langchain-huggingface --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3vL9TN8TWuj"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEst9m_R1ZPy"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"Q6nIsqJ4FX9XcZwlcQLSzA06z0oXYAGBw0jmCdbE\"\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus\")\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai_api_key = \"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\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_model2 = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "llm_model2 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "llm_model2 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
      ],
      "metadata": {
        "id": "0fWLusbOG27J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = \"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\"\n",
        "\n",
        "repo_id = \"tencent/Tencent-Hunyuan-Large\"\n",
        "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "llm_model3 = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    max_length=128,\n",
        "    temperature=0.5,\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvmHCzl1OQ8P",
        "outputId": "bb9a5ec0-8ed5-459a-f3c8-014fe4af76de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "Nv_mwGwkrlIh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqthzTFPZsvU"
      },
      "source": [
        "# Prompt tables in ChatGPT\n",
        "# we use Cohere instead of chatgpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljt5T8yPjYeq"
      },
      "source": [
        "# This is NOT working ❌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_VXS7tTjS4Y"
      },
      "source": [
        "# This is working ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "I7JxAG-Cf3A4",
        "outputId": "f91cc02e-c2bc-42e8-ff78-15bb58d4fb0e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "count = 10\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "CaUEtCcHrimT",
        "outputId": "220c6d84-a424-47e8-9faf-54c2d5492043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">20&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbrN-0pu5L3I"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "sojZoCdw59fF",
        "outputId": "fc630f3d-a878-43b7-adc5-191bff08be2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">30&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into French\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "output = llm_model.invoke(messages)\n",
        "\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPhryIPb6KOv",
        "outputId": "b428996a-3e2b-4d12-d3f3-56b1de39db86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour ! Comment puis-je vous aider aujourd'hui ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "EF8Gliua9Hs6",
        "outputId": "6a18bb9c-0ca7-4ffb-e75a-e57806ab9110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">40&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "m-ifyjPA6Fxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Types of LangChain Prompt Templates\n",
        "When you prompt in LangChain, you’re encouraged (but not required) to use a predefined template class such as:\n",
        "\n",
        "PromptTemplate for creating basic prompts.\n",
        "FewShotPromptTemplate for few-shot learning.\n",
        "ChatPromptTemplate for modeling chatbot interactions"
      ],
      "metadata": {
        "id": "Bi2JYnpi9C8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"What is the tallest mountain in the world?\",\n",
        "        \"answer\": \"Mount Everest\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the largest ocean on Earth?\",\n",
        "        \"answer\": \"Pacific Ocean\"\n",
        "     },\n",
        "    {\n",
        "        \"question\": \"In which year did the first airplane fly?\",\n",
        "         \"answer\": \"1903\"\n",
        "    },\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"Question: {question}\\n{answer}\",\n",
        ")\n",
        "\n",
        "prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"],\n",
        ")\n",
        "\n",
        "print('--------------------')\n",
        "\n",
        "\n",
        "print(\n",
        "    prompt_template.format(\n",
        "        input=\"What is the name of the famous clock tower in London?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print('--------------------')\n",
        "\n",
        "# Formatting the prompt with new content\n",
        "formatted_prompt = prompt_template.format(\n",
        "        input=\"What is the name of the famous clock tower in London?\"\n",
        "    )\n",
        "\n",
        "output = llm_model.invoke(formatted_prompt)\n",
        "\n",
        "print(output.content)\n"
      ],
      "metadata": {
        "id": "7LiNlVwyLN-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4a7bdd-3724-4f5a-accc-f0d38a6b52ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "Question: What is the tallest mountain in the world?\n",
            "Mount Everest\n",
            "\n",
            "Question: What is the largest ocean on Earth?\n",
            "Pacific Ocean\n",
            "\n",
            "Question: In which year did the first airplane fly?\n",
            "1903\n",
            "\n",
            "Question: What is the name of the famous clock tower in London?\n",
            "--------------------\n",
            "The famous clock tower in London is commonly known as 'Big Ben', although this is actually the name of the bell inside the tower, and not the tower itself. The tower is officially named 'Elizabeth Tower', and it has been a prominent landmark in the city of London since its completion in 1859. The clock tower is part of the Palace of Westminster, which is a UNESCO World Heritage Site and one of the most iconic buildings in the United Kingdom.\n",
            "\n",
            "Big Ben has become a symbol of London and is often featured in movies, television shows, and postcards, representing the city's rich history and cultural significance. The clock's accuracy and distinctive chimes have made it a global icon, and its image is instantly recognizable worldwide.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "_J_WqtVB6LvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "NUmMdR747GAP",
        "outputId": "14f6b2ad-d32f-4092-c402-9a5f5a8623ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">50&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define roles and placeholders\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "  [\n",
        "    (\"system\", \"You are a knowledgeable AI assistant. You are called {name}.\"),\n",
        "    (\"user\", \"Hi, what's the weather like today?\"),\n",
        "    (\"ai\", \"It's sunny and warm outside.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "   ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"Alice\", user_input=\"Can you tell me a joke?\")\n",
        "\n",
        "output = llm_model.invoke(messages)\n",
        "\n",
        "print(output.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0Lz1kbC6Opl",
        "outputId": "ab1e4c8f-9d5b-492d-bdeb-f6492848ce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a joke for you:\n",
            "\n",
            "Why did the chicken cross the road?\n",
            "To get to the other side!\n",
            "\n",
            "I hope that brought a smile to your face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "WKE_xUT69ifI",
        "outputId": "75364318-7a29-4e6d-f492-46830fa2387a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">60&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template2 = ChatPromptTemplate.from_messages([\n",
        "   (\"system\",\"You are a helpful AI Assistant with a sense of humor\"),\n",
        "   (\"human\",\"Hi how are you?\"),\n",
        "   (\"ai\",\"I am good. How can I help you?\"),\n",
        "   (\"human\",\"{user_input}\")\n",
        "])\n",
        "\n",
        "messages2 = chat_template2.format_messages(user_input=\"What is the capital of South Africa?\")\n",
        "\n",
        "# llm_model2.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "output2 = llm_model3.invoke(messages2)\n",
        "\n",
        "output2\n",
        "\n",
        "#print(output2.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CcB2YM4T9jc3",
        "outputId": "b6c0f791-5866-4b48-ea63-c8dda610e8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAI: The capital of South Africa is Pretoria.\\nUser '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Anaconda)",
      "language": "python",
      "name": "anaconda3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}