{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anumit-web/python-interview-2024/blob/main/machine%20learning/Unsupervised_Learning_clustering_type_iris_flower_clustering_Gaussian_mixture_model_(GMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lhvb-UXLbF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomuVIUl9uY6"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Shee7JuzVF"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "\n",
        "# web page summary\n",
        "\n",
        "# ChatPromptTemplate"
      ],
      "metadata": {
        "id": "99VCqplEvvAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating PromptTemplate with LLMs"
      ],
      "metadata": {
        "id": "8XwHCnNavxns"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1vTaKl9oHQ"
      },
      "source": [
        "# Chapter 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4QaixAfPyF"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKa4SWV84DMv"
      },
      "source": [
        "# Python Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "3pRBaqLz9atZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFXpB4wv-5-1",
        "outputId": "03ee5b24-0afb-4f9f-c729-34d7b4259fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0djESK29att",
        "outputId": "adbff87a-4405-4532-8bdc-3aefa0b7a5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "nfFMli_AEOK2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c3739b71-fb95-4d4b-fcfb-b5e57d06168f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2024-11-15  17:40'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "tz = timezone('EST')\n",
        "datetime.now(tz).strftime(\"%Y-%m-%d  %H:%M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDoARvnvU2L"
      },
      "source": [
        "---\n",
        "---\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR391vUc7l-N"
      },
      "source": [
        "# Python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfbOedi1WHO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "1SZd9nC1wLm3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import langchain\n",
        "except:\n",
        "  !pip install langchain\n",
        "  import langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "4iCA0l5hxjB8"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-community langchain-core --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "0n5IXOCNxppN"
      },
      "outputs": [],
      "source": [
        "# ! pip install --upgrade langchain --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -qU langchain-openai --quiet"
      ],
      "metadata": {
        "id": "_zVBLvkpHoUY"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "AWiheCyd1Yc0"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-cohere --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install langchain-huggingface --quiet"
      ],
      "metadata": {
        "id": "z_eJLyvxN3ax"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "A0RW7lT_UxHE"
      },
      "outputs": [],
      "source": [
        " ! pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "01GEQvpTUyyp"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-core langgraph>0.2.27 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "Ck7bfWqemvdt"
      },
      "outputs": [],
      "source": [
        "# ! pip install langchain-huggingface --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3vL9TN8TWuj"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "wEst9m_R1ZPy"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"Q6nIsqJ4FX9XcZwlcQLSzA06z0oXYAGBw0jmCdbE\"\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus\")\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai_api_key = \"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\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_model2 = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "llm_model2 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "llm_model2 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
      ],
      "metadata": {
        "id": "0fWLusbOG27J"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "Nv_mwGwkrlIh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqthzTFPZsvU"
      },
      "source": [
        "# Prompt tables in ChatGPT\n",
        "# we use Cohere instead of chatgpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljt5T8yPjYeq"
      },
      "source": [
        "# This is NOT working ❌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_VXS7tTjS4Y"
      },
      "source": [
        "# This is working ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "I7JxAG-Cf3A4",
        "outputId": "2f20141e-aec5-4c89-cc61-942a0793a93b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "count = 10\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbrN-0pu5L3I"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "sojZoCdw59fF",
        "outputId": "729b5903-f797-4531-de99-eb91a394fd45"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">20&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into French\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "output = llm_model.invoke(messages)\n",
        "\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPhryIPb6KOv",
        "outputId": "1c832106-95d5-4077-a1d3-1ab7a803e429"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour ! Comment puis-je vous aider aujourd'hui ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "EF8Gliua9Hs6",
        "outputId": "30f123de-c8d5-4699-9c7e-e8ec2e57be18"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">30&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "m-ifyjPA6Fxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "_J_WqtVB6LvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy"
      ],
      "metadata": {
        "id": "L4Zt9P7SdFon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "id": "WkMdnrjm4R13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "f09eee0c-6a0e-4eef-865c-63f5dd044313"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">40&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "2OfM3Jawd-FJ",
        "outputId": "1446013d-c180-46e0-ae95-20c480075c08"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">50&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count = count + 10\n",
        "counter_var = '<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">'+ str(count) + \"&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>\"\n",
        "display(HTML(counter_var))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "vl_IDNkQeBAV",
        "outputId": "ca1a86da-8eae-4f41-fd77-5e205ff202a6"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">60&nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"Q6nIsqJ4FX9XcZwlcQLSzA06z0oXYAGBw0jmCdbE\"\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus\")\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")"
      ],
      "metadata": {
        "id": "3hpmdqT3w6LZ"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "OwQSOcpeweGr"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into French\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "# Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\"),\n",
        "        (\"human\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")\n",
        "\n",
        "     ]\n",
        ")\n",
        "\n",
        "# Instantiate chain\n",
        "chain = create_stuff_documents_chain(llm_model, prompt)\n",
        "\n",
        "# Invoke chain\n",
        "result = chain.invoke({\"context\": docs})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQpaw8rmxPsI",
        "outputId": "9d685ecb-2cbb-4b2f-f5e5-f16e64ff84b5"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This article explores the potential of building autonomous agents with Large Language Models (LLMs) as their core controller. It highlights several proof-of-concept demos, such as AutoGPT, GPT-Engineer, and BabyAGI, which demonstrate the versatility of LLMs beyond text generation. The article discusses key components of an LLM-powered agent system, including planning, memory, and tool use. It also covers various techniques for task decomposition, self-reflection, and memory management, as well as the use of external tools to enhance the agent's capabilities. The article concludes by addressing some common challenges in building LLM-centered agents, such as finite context length, long-term planning, and the reliability of the natural language interface.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Travel blog"
      ],
      "metadata": {
        "id": "CpCKyBGa7Iyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://www.alikitravelblog.com/post/tarifa-travel-guide-spain\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "4GjOo-338bxp"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into French\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "# Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\"),\n",
        "        (\"human\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")\n",
        "\n",
        "     ]\n",
        ")\n",
        "\n",
        "# Instantiate chain\n",
        "chain = create_stuff_documents_chain(llm_model, prompt)\n",
        "\n",
        "# Invoke chain\n",
        "result = chain.invoke({\"context\": docs})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3o_oWa-8rDg",
        "outputId": "12a181b5-4f30-48d7-d078-fa7b5fa82802"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tarifa, a town in southern Spain, is a top destination for kitesurfing and beach holidays. Located on the Costa del Luz in Andalusia, it's accessible via nearby airports in Sevilla, Malaga, or the smaller Jerez. The town's unique geography creates ideal winds for kitesurfing, supported by a vibrant community and schools. Tarifa also offers windsurfing, rock climbing, mountain biking, horse riding, yoga, whale watching, and hiking. Accommodation options range from 5-star hotels to camping, with the latter being a local favourite. The Old Town boasts the best dining and nightlife, while nearby beaches offer great food and drink. Tarifa is also a convenient gateway to Morocco, with frequent ferries to Tangier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia page\n",
        "# Tylor Swift"
      ],
      "metadata": {
        "id": "w3DnLr3h--XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Taylor_Swift\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "Qn1Ze1_q-qPl"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into French\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "# Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\"),\n",
        "        (\"human\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")\n",
        "\n",
        "     ]\n",
        ")\n",
        "\n",
        "# Instantiate chain\n",
        "chain = create_stuff_documents_chain(llm_model, prompt)\n",
        "\n",
        "# Invoke chain\n",
        "result = chain.invoke({\"context\": docs})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "cunLZ4uw-z-8",
        "outputId": "7b00e442-d22a-401a-fa7d-39a6f0d1f86e"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "status_code: 400, body: {'message': 'too many tokens: size limit exceeded by 35032 tokens. Try using shorter or fewer inputs. The limit for this model is 128000 tokens.'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-a9346f06df66>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Invoke chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5352\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5353\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5354\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5355\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5356\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_cohere/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         )\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_generation_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m\"To suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcheck_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/base_client.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, message, accepts, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, return_prompt, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 )\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 raise BadRequestError(\n\u001b[0m\u001b[1;32m   1062\u001b[0m                     typing.cast(\n\u001b[1;32m   1063\u001b[0m                         \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: status_code: 400, body: {'message': 'too many tokens: size limit exceeded by 35032 tokens. Try using shorter or fewer inputs. The limit for this model is 128000 tokens.'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain # Import for summarization\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #Import for splitting if summarization not preferred\n",
        "from langchain.docstore.document import Document # Import Document class\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Initialize a summarization chain\n",
        "summarize_chain = load_summarize_chain(llm_model, chain_type=\"stuff\")\n",
        "# or\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "# docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# Summarize the documents before invoking the chain\n",
        "summarized_docs = summarize_chain.invoke(docs)\n",
        "# or\n",
        "# summarized_docs = docs # If using text splitter, assign docs back to summarized_docs\n",
        "\n",
        "#  Extract the summarized text and create a Document object\n",
        "summarized_text = summarized_docs['output_text'] # Extract the summarized text\n",
        "summarized_doc = Document(page_content=summarized_text) # Create a Document object\n",
        "\n",
        "# Invoke chain with the summarized document\n",
        "result = chain.invoke({\"context\": [summarized_doc]}) # Pass a list containing the Document object\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENNuPwtaAuCu",
        "outputId": "50eee446-6bc2-4d32-fd98-d234f04973d7"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taylor Swift, an acclaimed American singer-songwriter, has left an indelible mark on the music industry. Her autobiographical lyrics, constant artistic evolution, and powerful advocacy for artists' rights and women's empowerment have earned her widespread recognition. Swift's impressive discography boasts numerous commercially successful albums, and she has garnered an extensive list of accolades, including multiple Grammy Awards and recognition from esteemed publications. Her dedicated fan base, affectionately known as Swifties, further solidifies her cultural influence.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Anaconda)",
      "language": "python",
      "name": "anaconda3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}