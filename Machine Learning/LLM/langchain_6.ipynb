{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anumit-web/python-interview-2024/blob/main/machine%20learning/Unsupervised_Learning_clustering_type_iris_flower_clustering_Gaussian_mixture_model_(GMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lhvb-UXLbF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomuVIUl9uY6"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Shee7JuzVF"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jokes with LLM"
      ],
      "metadata": {
        "id": "Z3tkmTLuixNd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1vTaKl9oHQ"
      },
      "source": [
        "# Chapter 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4QaixAfPyF"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JolABC6StbU4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aq3-L4F4V5H"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1DHqE-Kh6H3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ-8TePAt4oW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKa4SWV84DMv"
      },
      "source": [
        "# Python Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pRBaqLz9atZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFXpB4wv-5-1",
        "outputId": "16ab2da5-63a5-4dc7-ef92-2adfa3bf7f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0djESK29att",
        "outputId": "bd4a645c-2fd8-44c7-831a-b6f5c0967872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfFMli_AEOK2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDoARvnvU2L"
      },
      "source": [
        "---\n",
        "---\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR391vUc7l-N"
      },
      "source": [
        "# Python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfbOedi1WHO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SZd9nC1wLm3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import langchain\n",
        "except:\n",
        "  !pip install langchain\n",
        "  import langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iCA0l5hxjB8"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-community langchain-core --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n5IXOCNxppN"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade langchain --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWiheCyd1Yc0"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-cohere --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0RW7lT_UxHE"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01GEQvpTUyyp"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-core langgraph>0.2.27 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3vL9TN8TWuj"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEst9m_R1ZPy"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"Q6nIsqJ4FX9XcZwlcQLSzA06z0oXYAGBw0jmCdbE\"\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus\")\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqthzTFPZsvU"
      },
      "source": [
        "# Get full contents of a web page and summarize it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljt5T8yPjYeq"
      },
      "source": [
        "# This is NOT working ❌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_VXS7tTjS4Y"
      },
      "source": [
        "# This is working ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7JxAG-Cf3A4",
        "outputId": "6a2c2d41-7698-44de-8417-b8061b096087"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOQ5VpBMf0p7",
        "outputId": "a001f41a-4744-4614-d692-2af7208ef99a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello Bob, I'm Command. How can I help you today?\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '8e40e8b1-dbb0-4b5f-b1d7-2a08dca7b2ea', 'token_count': {'input_tokens': 211.0, 'output_tokens': 14.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '8e40e8b1-dbb0-4b5f-b1d7-2a08dca7b2ea', 'token_count': {'input_tokens': 211.0, 'output_tokens': 14.0}}, id='run-06c492cc-dc54-4498-bfe0-0c3143fd1d90-0', usage_metadata={'input_tokens': 211, 'output_tokens': 14, 'total_tokens': 225})"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm_model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A48k1GtKf2fZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfdf6287-a792-4e82-ab44-ccd56835c33b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">20 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">20 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZCzGm1Oo5Ex",
        "outputId": "09f2bfff-8e99-4856-d2f2-91ebc012697e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Here\\'s a joke for you:\\n\\nWhy did the data scientist become a detective?\\nBecause they wanted to find the \"mean\" and \"classify\" the culprit!\\n\\nFeel free to ask for more jokes or any other data-related humor!', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c8c7320b-dd44-4370-878e-d7cf571b5ac9', 'token_count': {'input_tokens': 213.0, 'output_tokens': 52.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c8c7320b-dd44-4370-878e-d7cf571b5ac9', 'token_count': {'input_tokens': 213.0, 'output_tokens': 52.0}}, id='run-5a4ce47e-7a46-4d47-8569-050590bf9c22-0', usage_metadata={'input_tokens': 213, 'output_tokens': 52, 'total_tokens': 265})"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk--cmmkjRnx",
        "outputId": "bac1c476-dd5a-4afe-f16d-e8d586bd7abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure, here's a joke for you:\\n\\nWhy did the data scientist become a detective?\\nBecause they wanted to work on more data-intensive cases!\\n\\nI hope that brought a smile to your face!\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '33ccc338-2dbb-43fb-9260-6b8b2ec957d5', 'token_count': {'input_tokens': 213.0, 'output_tokens': 45.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '33ccc338-2dbb-43fb-9260-6b8b2ec957d5', 'token_count': {'input_tokens': 213.0, 'output_tokens': 45.0}}, id='run-db82b23f-c786-43f3-9ae1-764fa07a67b2-0', usage_metadata={'input_tokens': 213, 'output_tokens': 45, 'total_tokens': 258})"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist.\")])\n",
        "output.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWGyQyfjSEI",
        "outputId": "7d0c3843-35be-41f6-a3d4-c00e769b674d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, here's a joke for you:\\n\\nWhy did the data scientist become a detective?\\nBecause they wanted to work on more data-intensive cases!\\n\\nI hope you found that amusing! I can certainly provide more jokes if you'd like.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretty print\n",
        "\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IiWcEXyjSd-",
        "outputId": "3441f252-a4eb-4c48-f38b-a1eca99a2279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a joke for you:\n",
            "\n",
            "Why did the data scientist become a detective?\n",
            "Because they wanted to work on more data-intensive cases!\n",
            "\n",
            "I hope you found that amusing! I can certainly provide more jokes if you'd like.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5cZ5_J7YjS9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "tSOTr5G823HI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iN4h4fnw26j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">40 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DopXAnPkjTRm",
        "outputId": "72d474dc-d370-4919-8f9e-e1e230a28a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">40 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist. Tell me a joke about recruiter. Tell me a joke about psychologist.\")])\n",
        "\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "3lRjDtVFUY2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecffe7de-ba1c-44e8-bf9e-4ff3a049d220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the jokes as requested:\n",
            "\n",
            "**Data Scientist:**\n",
            "Why did the data scientist break up with their partner? Because they couldn't find a correlation between their love and happiness.\n",
            "\n",
            "**Recruiter:**\n",
            "What did the recruiter say to the candidate who was late for the interview? \"We're looking for someone with excellent time management skills, and you've just shown us you're not that person!\"\n",
            "\n",
            "**Psychologist:**\n",
            "Why did the psychologist stop playing poker with his friends? Because he kept folding, even when he had a full house. His friends couldn't understand why he was so indecisive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "qbrN-0pu5L3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Prompt Templates for LLMs in LangChain"
      ],
      "metadata": {
        "id": "QHQizCD84teq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">100 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RClzFiic5sml",
        "outputId": "8f7a03c1-cfb0-4c50-a444-79804c66f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">100 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")\n",
        "\n",
        "USER_INPUT = 'Paris'\n",
        "\n",
        "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "\n",
        "input_variables=[\"location\"],\n",
        "\n",
        "template=template,\n",
        "\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location=USER_INPUT )\n",
        "\n",
        "# print(f\"LLM Output: {llm_model(final_prompt)}\")\n",
        "\n",
        "\n",
        "# Wrap the final_prompt in a HumanMessage object\n",
        "message = HumanMessage(content=final_prompt)\n",
        "\n",
        "# Call llm_model with the message object\n",
        "print(f\"LLM Output: {llm_model([message])}\") # Call with a list containing the HumanMessage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfCgKdwL6JKf",
        "outputId": "b7c8445b-44a9-4495-91e2-b422de939f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output: content=\"- Visit the Eiffel Tower: Take a trip to the iconic Eiffel Tower, a symbol of Paris and one of the most famous landmarks in the world. Purchase tickets in advance to access the summit for breathtaking views of the city. While you're there, explore the different levels, enjoy a meal at the on-site restaurants, and learn about the tower's history at the exhibits.\\n\\n- Explore the Louvre Museum: Spend a day at the Louvre, the world's largest art museum. Admire masterpieces such as the Mona Lisa, the Venus de Milo, and the Winged Victory of Samothrace. The museum's vast collection spans centuries and includes ancient artifacts, sculptures, paintings, and drawings. Don't miss the stunning glass pyramid entrance designed by I.M. Pei.\\n\\n- Stroll along the Avenue des Champs-Élysées: Wander down this famous avenue, known for its high-end boutiques, luxury shops, and elegant cafes. Start from the Place de la Concorde, admire the beautiful architecture, and perhaps enjoy a coffee or a meal at one of the renowned restaurants. You can also visit the Arc de Triomphe at the avenue's end, offering a panoramic view of Paris and a glimpse into the city's rich history.\" additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'acd2fb6e-a3dc-43fb-9f2f-37fe6ddaf5bf', 'token_count': {'input_tokens': 237.0, 'output_tokens': 248.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'acd2fb6e-a3dc-43fb-9f2f-37fe6ddaf5bf', 'token_count': {'input_tokens': 237.0, 'output_tokens': 248.0}} id='run-1c7bc541-0da2-4090-9133-b066e51e6f46-0' usage_metadata={'input_tokens': 237, 'output_tokens': 248, 'total_tokens': 485}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_model([message])"
      ],
      "metadata": {
        "id": "TIlvhuPh_Vtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU2VcW3VTkmC",
        "outputId": "7c7959d1-b322-4a4b-c5ad-2796a93d0e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Visit the Eiffel Tower: Take a trip to the iconic Eiffel Tower, one of the most famous landmarks in the world. Purchase tickets in advance to access the summit for breathtaking views of the city. While you're there, explore the different levels, enjoy a meal at one of the tower's restaurants, and learn about the history of its construction.\n",
            "\n",
            "- Explore the Louvre Museum: Spend a day at the world's largest art museum, the Louvre. Admire masterpieces like the Mona Lisa, the Venus de Milo, and the Winged Victory of Samothrace. Join a guided tour to gain deeper insights into the vast collection, and don't miss the opportunity to see the impressive glass pyramid entrance designed by I.M. Pei.\n",
            "\n",
            "- Stroll along the Avenue des Champs-Élysées: Wander down this famous avenue, known for its high-end boutiques, theatres, and cafes. Start from the Place de la Concorde, admire the Arc de Triomphe at the other end, and stop by the luxurious shops like Louis Vuitton, Cartier, and Gucci. Enjoy the vibrant atmosphere, and if you're visiting in winter, experience the magical Christmas market that lines the avenue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining LLMs and Prompts in Multi-Step Workflows"
      ],
      "metadata": {
        "id": "D9Fcy4QoTyiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")\n",
        "\n",
        "template = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
        "\n",
        "first_prompt = PromptTemplate(\n",
        "\n",
        "input_variables=[\"country\"],\n",
        "\n",
        "template=template)\n",
        "\n",
        "chain_one = LLMChain(llm = llm_model, prompt = first_prompt)\n",
        "\n",
        "# second step in chain\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "\n",
        "input_variables=[\"city\"],\n",
        "\n",
        "template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\",)\n",
        "\n",
        "chain_two = LLMChain(llm=llm_model, prompt=second_prompt)\n",
        "\n",
        "# Combine the first and the second chain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
        "\n",
        "final_answer = overall_chain.invoke(\"Canada\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxbmdZiVV5jC",
        "outputId": "42e0ef54-3029-4240-9551-bc6d19881153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mToronto.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m- Visit the CN Tower: This iconic landmark offers breathtaking views of the city from its observation decks and is an architectural marvel.\n",
            "- Explore the Royal Ontario Museum (ROM): With an extensive collection of art, culture, and natural history exhibits, ROM provides an immersive learning experience.\n",
            "- Stroll through Distillery District: Known for its well-preserved Victorian-era architecture, this pedestrian-only district is filled with art galleries, unique shops, and excellent restaurants.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Anaconda)",
      "language": "python",
      "name": "anaconda3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}