{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anumit-web/python-interview-2024/blob/main/machine%20learning/Unsupervised_Learning_clustering_type_iris_flower_clustering_Gaussian_mixture_model_(GMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_lhvb-UXLbF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomuVIUl9uY6"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Shee7JuzVF"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jokes with LLM"
      ],
      "metadata": {
        "id": "Z3tkmTLuixNd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1vTaKl9oHQ"
      },
      "source": [
        "# Chapter 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4QaixAfPyF"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JolABC6StbU4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aq3-L4F4V5H"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1DHqE-Kh6H3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ-8TePAt4oW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKa4SWV84DMv"
      },
      "source": [
        "# Python Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "3pRBaqLz9atZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFXpB4wv-5-1",
        "outputId": "a992e425-9ef2-44b9-b3f9-617789967a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0djESK29att",
        "outputId": "dd7f8a7b-57ad-4d48-e39a-90bb9bd9aec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "nfFMli_AEOK2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDoARvnvU2L"
      },
      "source": [
        "---\n",
        "---\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR391vUc7l-N"
      },
      "source": [
        "# Python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfbOedi1WHO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "1SZd9nC1wLm3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import langchain\n",
        "except:\n",
        "  !pip install langchain\n",
        "  import langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "4iCA0l5hxjB8"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-community langchain-core --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "0n5IXOCNxppN"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade langchain --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "AWiheCyd1Yc0"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-cohere --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "A0RW7lT_UxHE"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "01GEQvpTUyyp"
      },
      "outputs": [],
      "source": [
        "! pip install langchain-core langgraph>0.2.27 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3vL9TN8TWuj"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "wEst9m_R1ZPy"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"Q6nIsqJ4FX9XcZwlcQLSzA06z0oXYAGBw0jmCdbE\"\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus\")\n",
        "\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqthzTFPZsvU"
      },
      "source": [
        "# Get full contents of a web page and summarize it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljt5T8yPjYeq"
      },
      "source": [
        "# This is NOT working ❌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_VXS7tTjS4Y"
      },
      "source": [
        "# This is working ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "I7JxAG-Cf3A4",
        "outputId": "987fc5b0-4cd9-4ea4-fa25-268e0f6e0520"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">10 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOQ5VpBMf0p7",
        "outputId": "ede93e65-2e4e-49fc-ef10-4d21c3be7cef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello Bob! It's a pleasure to meet you. How can I assist you today?\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '6759409f-a860-4a1d-bf6c-3d02ce4ff1bc', 'token_count': {'input_tokens': 211.0, 'output_tokens': 18.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '6759409f-a860-4a1d-bf6c-3d02ce4ff1bc', 'token_count': {'input_tokens': 211.0, 'output_tokens': 18.0}}, id='run-746303aa-2229-4df0-87fe-5c1f8d5bf93d-0', usage_metadata={'input_tokens': 211, 'output_tokens': 18, 'total_tokens': 229})"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm_model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "A48k1GtKf2fZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "e263e95b-2820-41b2-ba93-128e870948aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">20 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">20 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZCzGm1Oo5Ex",
        "outputId": "d6a36570-ad31-4ead-ff5a-20312f8b1820"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure, here's a joke for you:\\n\\nWhy did the data scientist become a detective?\\nBecause they wanted to work on more data-intensive cases!\\n\\nI hope that brought a smile to your face!\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'f72ae50d-d38e-4000-8fbd-abf14e16d486', 'token_count': {'input_tokens': 213.0, 'output_tokens': 45.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'f72ae50d-d38e-4000-8fbd-abf14e16d486', 'token_count': {'input_tokens': 213.0, 'output_tokens': 45.0}}, id='run-20d0da9b-6095-4c69-96ba-f507b81aef9e-0', usage_metadata={'input_tokens': 213, 'output_tokens': 45, 'total_tokens': 258})"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk--cmmkjRnx",
        "outputId": "bb6bb7e1-4fcb-4c8e-cad2-bc5aba250484"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure, here's a joke for you:\\n\\nWhy did the data scientist become a detective?\\nBecause they wanted to work on more data-intensive cases!\\n\\nI hope that brought a smile to your face!\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '0cea3304-a649-46e8-9088-69eede75d37e', 'token_count': {'input_tokens': 213.0, 'output_tokens': 45.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '0cea3304-a649-46e8-9088-69eede75d37e', 'token_count': {'input_tokens': 213.0, 'output_tokens': 45.0}}, id='run-a491e3b7-8270-4159-93b6-fd9a31bad8e3-0', usage_metadata={'input_tokens': 213, 'output_tokens': 45, 'total_tokens': 258})"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist.\")])\n",
        "output.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "8oWGyQyfjSEI",
        "outputId": "861df957-fb74-43d4-9398-33045b0cd0ec"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, here's a joke about data scientists:\\n\\nWhy did the data scientist become a detective?\\nBecause they were great at finding missing values!\\n\\nThis joke plays on the idea that data scientists often work with incomplete datasets and use various techniques to handle missing data, so they could be seen as 'detectives' of sorts in their field.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretty print\n",
        "\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IiWcEXyjSd-",
        "outputId": "0fb8ad48-5c11-47cd-dcfb-1f6a0ffcc94e"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a joke about data scientists:\n",
            "\n",
            "Why did the data scientist become a detective?\n",
            "Because they were great at finding missing values!\n",
            "\n",
            "This joke plays on the idea that data scientists often work with incomplete datasets and use various techniques to handle missing data, so they could be seen as 'detectives' of sorts in their field.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5cZ5_J7YjS9O"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "tSOTr5G823HI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iN4h4fnw26j_"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">40 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "DopXAnPkjTRm",
        "outputId": "7b27d4fa-e0db-4f85-c21f-5b2ce926144e"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">40 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm_model.invoke([HumanMessage(content=\"Tell me a joke about data scientist. Tell me a joke about recruiter. Tell me a joke about psychologist.\")])\n",
        "\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "3lRjDtVFUY2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028e2d0d-194c-41e8-f219-e1aa5312116c"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the jokes as requested:\n",
            "\n",
            "**Data Scientist:**\n",
            "Why did the data scientist break up with their partner? Because they couldn't find a correlation between their love and happiness.\n",
            "\n",
            "**Recruiter:**\n",
            "What did the recruiter say to the candidate who was late for the interview? \"We're looking for someone with excellent time management skills, and you've just shown us that you might not be the best fit!\"\n",
            "\n",
            "**Psychologist:**\n",
            "Why did the psychologist prefer treating goldfish? Because they believed that a fish's memory only lasts a few seconds, so the fish would never remember if the psychologist made any mistakes during the therapy session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "qbrN-0pu5L3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Prompt Templates for LLMs in LangChain"
      ],
      "metadata": {
        "id": "QHQizCD84teq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">100 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "RClzFiic5sml",
        "outputId": "1cdaee36-a661-4033-8877-43e660f705d5"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color:#DE0000;font-size:60px;font-weight: 900;\">100 &nbsp; &nbsp;  &nbsp; &nbsp; &#8595;</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "llm_model = ChatCohere(model=\"command-r-plus-08-2024\")\n",
        "\n",
        "USER_INPUT = 'Paris'\n",
        "\n",
        "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "\n",
        "input_variables=[\"location\"],\n",
        "\n",
        "template=template,\n",
        "\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location=USER_INPUT )\n",
        "\n",
        "# print(f\"LLM Output: {llm_model(final_prompt)}\")\n",
        "\n",
        "\n",
        "# Wrap the final_prompt in a HumanMessage object\n",
        "message = HumanMessage(content=final_prompt)\n",
        "\n",
        "# Call llm_model with the message object\n",
        "print(f\"LLM Output: {llm_model([message])}\") # Call with a list containing the HumanMessage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfCgKdwL6JKf",
        "outputId": "db6c0fef-bb23-472d-e0af-1e7523103f8b"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output: content=\"- Visit the Eiffel Tower: Take a trip to the iconic Eiffel Tower, one of the most famous landmarks in the world. Purchase tickets in advance to access the summit for breathtaking panoramic views of the city. While you're there, explore the different levels, learn about the tower's history, and enjoy a meal at one of the on-site restaurants, such as the Michelin-starred Le Jules Verne.\\n\\n- Explore the Louvre Museum: Home to an extensive collection of art and historical artifacts, the Louvre is a must-visit destination for art enthusiasts. View renowned masterpieces like the Mona Lisa, the Venus de Milo, and the Winged Victory of Samothrace. Consider joining a guided tour to navigate the vast museum efficiently and gain insights into the highlights of the collection.\\n\\n- Stroll along the Avenue des Champs-Élysées: This famous avenue is known for its high-end boutiques, luxury brands, and elegant cafes. Start from the Place de la Concorde, admire the architecture, and indulge in some luxury shopping. Stop by the Arc de Triomphe at the end of the avenue, and if time permits, climb to the top for another stunning view of Paris.\" additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'a39011cb-e84c-4b0b-9648-5e058d3a2a35', 'token_count': {'input_tokens': 237.0, 'output_tokens': 235.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'a39011cb-e84c-4b0b-9648-5e058d3a2a35', 'token_count': {'input_tokens': 237.0, 'output_tokens': 235.0}} id='run-2b59c512-3994-46c4-bf92-96e50782684b-0' usage_metadata={'input_tokens': 237, 'output_tokens': 235, 'total_tokens': 472}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Anaconda)",
      "language": "python",
      "name": "anaconda3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}